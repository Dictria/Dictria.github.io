---
layout: post
title: "MPI"
subtitle: "MPI笔记"
date: 2021-12-15
author: "Dictria"
header-img: "img/header.jpg"
tags: 
  - HPC
  - MPI
---


## 1. MPI Outline
```cpp
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    //Initialize MPI environment
    MPI_Init(NULL, NULL);
    
    //Get the number of processes
    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    //Get rank of the process
    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    //Get the name of the processor
    char processor_name[MPI_MAX_PROCESSOR_NAME];
    int name_len;
    MPI_Get_proocessor_name(processor_name, &name_len);

    printf("Hello world from processor %s, rank %d out of %d processors\n", processor_name, world_rank, world_size);

    MPI_Finalize();

    return 0;
}
```
* MPI environment must be initialized with: 
```cpp
MPI_init(int* argc, char** argv);
```
  * During this process, all of MPI's global and internal variables are constructed. 
  * `MPI_Init()` takes two arguments that are **not necessary**. And extra parameters are simply left as extra space in case future implementation might need them. 

* There are two functions are used in almost every MPI program
```cpp
MPI_Comm_size(MPI_Comm communicator, int* size);
```
`MPI_Comm_size`: Return the size of a communicator. In the example, `MPI_COMM_WORLD`(constructed by MPI) encloses all of the processes in the job, so this call should return the amount of processes that were requested for the job.    

```cpp
MPI_Comm_rank(MPI_Comm communicator, int* rank);
```
`MPI_Comm_rank` returns the rank of a process in a communicator. Each process inside of a communicator is assiged an incremental rank starting from zero which are used for identification purpose when sending and receiving message.   

* a less-used fuction in the program   
```cpp
MPI_Get_processor_name(char* name, int* name_length);
```
`MPI_Get_processor_name()` obtains the actual name of the processor on which the process is executing.

* the final call in the program is:   
```cpp
MPI_Finalize();
```
`MPI_Finalize()` is used to clean up the MPI environment. **No more MPI call can be made after this function.**    

### Running the MPI sample
```c
mpicc -o mpi_hello_world mpi_hello_world.c
mpic++ -o mpi_test mpi_test.cpp
//specify the number of process
mpirun -n 4 ./mpi_hello_world
```

## 2. MPI Send and Receive
```cpp
MPI_Send(void* data, int count, MPI_Datatype datatype, int destination, int tag, MPI_Comm communicator);

MPI_Recv(void* data, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm communicator, MPI_Status* status);
```
* data: the data buffer
* count: count of element in the data buffer. `MPI_Send` sends the exact count of elements, and `MPI_Recv` will receive **at most** the count of elements.
* datatype: type of element in the data buffer   
* destination/source: the rank of the process
* tag: message tag
* status: information about the received message

### 2.1 Elementary MPI datatypes
* The `MPI_Send` and `MPI_Recv` functions utilize MPI Datatypes as a means to specify the structure of a message.    
* MPI Datatype and equivalent C datatypes:
|MPI datatype|C equivalent|
|:----:|:----:|
|MPI_SHORT|short|
|MPI_INT|int|
|MPI_LONG|long|
|MPI_LONG_LONG|long long|
|MPI_UNSIGNED_CHAR|unsigned char|
|MPI_UNSIGNED_SHORT|unsigned short|
|MPI_UNSIGNED|unsigned int|
|MPI_UNSIGNED_LONG|unsigned long|
|MPI_UNSIGNED_LONG_LONG|unsigned long long|
|MPI_FLOAT|float|
|MPI_DOUBLE|double|
|MPI_LONG_DOUBLE|long double|
|MPI_BYTE|char|

### 2.2 MPI send/recv program
```cpp
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>

int main(int argc, char** args) {
  MPI_Init(NULL, NULL);
  int world_rank;
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
  int world_size;
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);

  if(world_size < 2) {
    fprintf(stderr, "Woorld size must be greater than 1 for %s\n", argv[0]);
    MPI_Abort(MPI_COMM_WORLD, 1);
  }

  int number;
  if(world_rank == 0) {
    number = -1;
    MPI_Send(&number, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);
  }else if(world_rank == 1) {
    MPI_Recv(&number, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNONRE);
    printf("Process 1 received number %d from process 0\n", number);
  }
  MPI_Finalize();
}
```

### 2.3 MPI ping pong program
```cpp
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>

int main(int argc, char** argv) {
  const int PING_PONG_LIMIT = 10;

  MPI_Init(NULL, NULL);
  int world_rank;
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
  int world_size;
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);

  if (world_size != 2) {
    fprintf(stderr, "World size must be two for %s\n", argv[0]);
    MPI_Abort(MPI_COMM_WORLD, 1);
  }

  int ping_pong_count = 0;
  int partner_rank = (world_rank + 1) % 2;
  while (ping_pong_count < PING_PONG_LIMIT) {
    if (world_rank == ping_pong_count % 2) {
      // Increment the ping pong count before you send it
      ping_pong_count++;
      MPI_Send(&ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD);
      printf("%d sent and incremented ping_pong_count %d to %d\n",
             world_rank, ping_pong_count, partner_rank);
    } else {
      MPI_Recv(&ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD,
               MPI_STATUS_IGNORE);
      printf("%d received ping_pong_count %d from %d\n",
             world_rank, ping_pong_count, partner_rank);
    }
  }
  MPI_Finalize();
}
```

## 3. Dynamic Receiving with MPI Probe and Status
### 3.1 MPI_Status
* The `MPI_Recv` operation takes the address of an `MPI_Status` structure as an argument(can be ignored with `MPI_STATUS_IGNORE`). If pass an `MPI_Status` structure to the `MPI_Recv` function, it will be populated with addition information about the receive operation.
* The three primary pieces of information include:
  1. The rank of the sender: store in the `MPI_SOURCE` element of the structure.
  2. The tag of the message: `MPI_TAG`
  3. The length of the message: does not have a predefied element in the status structure, but use `MPI_Get_count(MPI_Status* status, MPI_Datatype datatype, int* count)`, the length contains in the argument `count`

* `MPI_Recv` can take `MPI_ANY_SOURCE` for the rank of the sender and `MPI_ANY_TAG` for  the tag of the message. In this case, `MPI_Status` is the only way to find out these information.
* The return value from `MPI_Get_count` is relative to the datatype which is passed. e.g. `MPI_CHAR` as datatype will be four times larger than the count using `MPI_INT`.  

### 3.2 MPI_Probe
* You can use `MPI_Probe` to query the message size before actually receiving it. The function prototype looks like this:
```cpp
MPI_Probe(int source, int tag, MPI_Comm comm, MPI_Status* status);
```
* `MPI_Probe` does everything as `MPI_Recv` but receiving the message.
* `MPI_Probe` will block for a message with a matching tag and sender. When the message is available, it will fill the status structure with iinformation. The user can then use `MPI_Recv` to receive the actual message.   
```cpp
int number_amount;
if (world_rank == 0) {
    const int MAX_NUMBERS = 100;
    int numbers[MAX_NUMBERS];
    // Pick a random amount of integers to send to process one
    srand(time(NULL));
    number_amount = (rand() / (float)RAND_MAX) * MAX_NUMBERS;

    // Send the random amount of integers to process one
    MPI_Send(numbers, number_amount, MPI_INT, 1, 0, MPI_COMM_WORLD);
    printf("0 sent %d numbers to 1\n", number_amount);
} else if (world_rank == 1) {
    MPI_Status status;
    // Probe for an incoming message from process zero
    MPI_Probe(0, 0, MPI_COMM_WORLD, &status);

    // When probe returns, the status object has the size and other
    // attributes of the incoming message. Get the message size
    MPI_Get_count(&status, MPI_INT, &number_amount);

    // Allocate a buffer to hold the incoming numbers
    int* number_buf = (int*)malloc(sizeof(int) * number_amount);

    // Now receive the message with the allocated buffer
    MPI_Recv(number_buf, number_amount, MPI_INT, 0, 0,
             MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    printf("1 dynamically received %d numbers from 0.\n",
           number_amount);
    free(number_buf);
}
```

## 4. Point-to-Point Communication Application - Random Walk

## 5. MPI Broadcast and Collective Communication
* `MPI_Barrier(MPI_Comm communicator)`: create a barrier

### 5.1 Broadcasting with MPI_Bcast
* During a broadcast, one process sends the same data to all processes in a communicator. 
* One of the main uses of broadcasting is to send out user input to a parallel program or send out configuration parameters to all processes.
* Broadcasting can be accomplished by using `MPI_Bcast`:   
```cpp
MPI_Bcast(void* data, int count, MPI_Datatype datatype, int root, MPI_Comm communicator);
```
* The root process and receiver processes all call the same `MPI_Bcast()` function. When all of the receiver processes call `MPI_Bcast`, the data variable will be filled in with the data.   

* `MPI_Wtime()`: takes no arguments, and it simply return a floating-point number of **seconds** since a set time in the past. Return a double type.

## 6. MPI Scatter, Gather and Allgather

### 6.1 MPI_Scatter
* It is a collective routine that is very similar to `MPI_Bcast()`.
* `MPI_Bcast()` sends the same piece of data to all processes while `MPI_Scatter()` sends chunks of an array to different processes.
```cpp
MPI_Scatter(void* data, int send_count, MPI_Datatype send_datatype, void* recv_data, int recv_count, MPI_Datatype recv_datatype, int root, MPI_Comm communicator);
```
* `send_count` dictate how many elements of a specific MPI Datatype will be sent to each process.

### 6.2 MPI_Gather
* `MPI_Gather` is the inverse of MPI_Scatter. It takes elements from each process and gathers them to the root procecss. The elements are ordered by the rank of the process from which they were received
```cpp
MPI_Gather( void* send_data, int send_count, MPI_Datatype send_datatype, void* recv_data, int recv_count, MPI_Datatype recv_datatype, int root, MPI_Comm communicator);  
```
* In `MPI_Gather`, only the root process needs to have a valid receive buffer. All other calling processes can pass `NULL` for `recv_data`.
* The `recv_count` parameter is the count of elements received per process, not total summation of counts from all processes.

## 6.3 MPI_Alltogather
* `MPI_Alltogether` is  a many-to-many communication pattern. Given a set of elements distributed across all proocesses, `MPI_Alltogether` will gather all of the elements to all the processes. 
```cpp
MPI_Allgather( void* send_data, int send_count, MPI_Datatype send_datatype, void* recv_data, int recv_count, MPI_Datatype recv_datatype, MPI_Comm communicator);
// just no root argument compared to MPI_Gather
```

## 7. MPI Reduce and Allreduce
### 7.1 MPI_Reduce
* `MPI_Reduce` takes an array of input elements on each process and returns an array of output elements to the root process. The output elements contains the reduced result.
```cpp
MPI_Reduce( void* send_data, void* recv_data, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm communicator);
```
* The `recv_data` is only relevant on the process with a rank of `root`, The `recv_data` array contains the reduced result and has a size of `count`.
* The reduction operations defined by MPI:   
|MPI_Op|Explanation|    
|:----:|:----:|    
|MPI_MAX|maximum element|    
|MPI_MIN|minimum element|    
|MPI_SUM|sums the elements|    
|MPI_PROD|multiplies all elements|    
|MPI_LAND|perform a logical and across the elements|    
|MPI_LOR|perform a logical and across the elements|    
|MPI_BAND|perform a bitwise and across the bits of the elements|    
|MPI_BOR|perform a bitwise or across the bits of the elements.|    
|MPI_MAXLOC|Returns the maximum value and the rank of the process that owns it|    
|MPI_MINLOC|Returns the minimum value and the rank of the process that owns it|    

![](/img/in-post/2021-12-15-mpi/reduce.png)   

### 7.2 MPI_Allreduce
* `MPI_Allreduce` will reduce the values and distribute the results to all processes. 
```cpp
MPI_Allreduce(void* send_data, void* recv_data, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm communicator);
```

## 8. MPI Groups and Communicators
### 8.1 Communicators
#### MPI_Comm_split MPI_Comm_free
* Function to create new communicator:   
```cpp
MPI_Comm_split( MPI_Comm comm, int color, int key, MPI_Comm* newcomm);
```
* `MPI_Comm_split` creates new communicators by "splitting" a communicator into a group of sub-communicators based on the input values `color` and `key`. **The original communicator doesn't go away**, but a new communicator is created on each process.
* `comm` is the communicator that will be used as the basis for the new communicators.  
* `color` determines to which new communicator each processes will belong. All process which pass in the same value for `color` are assigned to the same communicator. If the `color` is `MPI_UNDEFINED`, that process won't be included in any of the new communicators.
* `order` determines the ordering within each new communicator. The process which passes in the smallest value for `key` will be rank 0, and so on.
&nbsp;
* Finally, free the Communicator with `MPI_Comm_free(MPI_Comm* comm)`. 

#### Other communicataor creation functions
* `MPI_Comm_dup`: creates a duplicate of a communicator to avoid user code and library interfere with each other.
* `MPI_Comm_create`: 
```cpp
MPI_Comm_create(MPI_Comm comm, MPI_Group group, MPI_Comm* newcomm);
```

### 8.2 Group
* **A communicator contains a context and a group**

* `MPI_Comm_group`: get the group of processes in a comminicator
```cpp
MPI_Comm_group(MPI_Comm comm, MPI_Group* group);
```
* `MPI_Group_rank` and `MPI_Group_size`: get the rank and size for the group.   
* **Local Operation** and **Remote Operation**: a remote operation involves communication withe other ranks where a local operation does not.
* Creating a new communicator is a remote operation because all processes need to decide on the same context and group, where creating a group is local because it isn’t used for communication and therefore doesn’t need to have the same context for each process. You can manipulate a group all you like without performing any communication at all.
* Group operations:    
```cpp
MPI_Group_union(MPI_Group group1, MPI_Group group2, MPI_Group* newgroup);

MPI_Group_intersection(MPI_Group group1, MPI_Group group2, MPI_Group* newgroup);
```

* `MPI_Comm_create_group`: create a new communicator.    
```cpp
MPI_Comm_create_group(MPI_Comm comm, MPI_Group group, int tag, MPI_Comm* newcomm)
```

* `MPI_Group_incl`: pick specific ranks in a group and construct a new group containing only those ranks;   
```cpp
MPI_Group_incl(MPI_Group group, int n, const int ranks[], MPI_Group* newgroup)
```

