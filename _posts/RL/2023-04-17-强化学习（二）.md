---
layout: post
title: "强化学习（二）"
subtitle: "强化学习笔记"
date: 2023-04-17
author: "Dictria"
header-img: "img/header.jpg"
mathjax: true
tags: 
  - 强化学习
  - AI
---



## 2. 多臂赌博机(MAB)，无状态

* 定义：重复在k个选项或动作进行选择，每次做出选择之后，都会得到一定数值的收益，收益由选择的动作决定的平稳概率分布产生。目标是在某一段时间内最大化总收益期望。
* **开发**：选取价值最高的动作
* **试探**：选择非价值最高的动作
* 开发和试探平衡

### 2.1 动作价值方法

* 使用动作的价值的估计来进行动作选择
* 采样平均方法的价值估计：$Q_t(a)=\frac{t时刻前通过执行动作a得到的收益总和}{t时刻前执行动作a的次数}$
  * 根据大数定律，**价值估计**最终会收敛到**价值的真实值**
* 贪心动作：选择具有最高估计值的动作：$A_t=\underset{a}{argmax}Q_t(a)$
* $\epsilon$-贪心方法：以$\epsilon$概率从所有动作中等概率选择，以$1-\epsilon$概率贪心

### 2.2 10臂测试平台

* 总体上$\epsilon$-贪心优于纯贪心
* $\epsilon$-贪心，$\epsilon$越大，收敛越快。$\epsilon$越小，极限效果越好。

### 2.3 增量式实现

* $新估计值\leftarrow 旧估计值+步长\times [目标-估计值]$，步长为$\alpha$
* 采样平均方法的价值估计：$Q_{n+1}=Q_n+\frac{1}{n}[R_n-Q_n]$

### 2.4 跟踪非平稳过程

* 采样平均不适用于非平稳过程

* 加权平均：$Q_{n+1}=Q_n+\alpha [R_n-Q_n]$
* 收敛概率为1的条件：$\sum_{n=1}^{\infty}\alpha_n = \infty且\sum_{n=1}^{\infty}\alpha_n^2<\infty$

### 2.5 乐观初始值

* 设置过分乐观的动作初始价值，做出选择后，系统会感到”失望“，按照贪心法选择动作，系统也会进行大量试探。

### 2.6 基于置信度上界(Upper confidence bound,UCB)的动作选择

* $\epsilon$-贪心是一种盲目选择，在非贪心动作中，最好根据他们的潜力来选择可能事实上是最优的动作

* 基于置信度上界的动作选择：$A_t=\underset{a}{argmax}[Q_t(a)+c\sqrt{\frac{lnt}{N_t(a)}}]$。$c$是一个大于0的数，控制试探程度

  * 思想：平方根项是对a动作估计的不确定或方差的度量

* UCB算法相比$\epsilon$-算法更难推广到更一般的强化学习问题

### 2.7 梯度赌博机算法

* 偏好函数$H_t(a)$，偏好函数越大，动作选择越频繁
* softmax确定的动作概率：$\frac{e^{H_t(a)}}{\sum_{b=1}^ke^{H_t(b)}}=\pi_t(a)$
* $\pi_t(a)$表示动作a在t时刻被选择的概率
* 在每个步骤，选择动作$A_t$并获得收益$R_t$后，偏好函数按如下方式更新：
  * $H_{t+1}(A_t)=H_t(A_t)+\alpha (R_t-\bar{R_t})(1-\pi_t(A_t))$
  * $H_{t+1}(a)=H_t(a)-\alpha (R_t-\bar{R_t})\pi_t(a)$，对于$a \neq A_t$
  * $\bar{R_t}$是t时刻所有收益的平均值，基准项